# Flappy Bird AI com Q-Learning  
**Nome do Projeto**: FlappyBirdQLearning-01656165-StaloneAugusto  

## üéØ Introdu√ß√£o ao Projeto  
Este projeto implementa um agente de Intelig√™ncia Artificial utilizando **Q-Learning** para jogar o cl√°ssico *Flappy Bird* autonomamente. O objetivo √© demonstrar como algoritmos de aprendizado por refor√ßo podem ser aplicados em ambientes de jogos, permitindo que o agente aprenda a tomar decis√µes √≥timas (pular ou n√£o pular) para maximizar sua pontua√ß√£o.  

O jogo foi desenvolvido em **Python** com a biblioteca **Pygame**, e o agente utiliza uma **tabela Q** para armazenar e atualizar suas experi√™ncias, aprendendo atrav√©s de tentativa e erro com um sistema de recompensas/puni√ß√µes.  

---

## üõ† Tecnologias e Bibliotecas Utilizadas  
- **Python 3.x** (Linguagem principal)  
- **Pygame** (Renderiza√ß√£o do jogo e interface gr√°fica)  
- **NumPy** (C√°lculos num√©ricos e manipula√ß√£o de arrays)  
- **Matplotlib** (Gera√ß√£o de gr√°ficos de desempenho)  
- **Collections** (Implementa√ß√£o eficiente da tabela Q com `defaultdict`)  

**Instala√ß√£o das depend√™ncias**:  
```bash
pip install pygame numpy matplotlib
```

---

## üß† Algoritmos Aplicados  
### **Q-Learning**  
- **Pol√≠tica Œµ-greedy**: Balanceia **explora√ß√£o** (a√ß√µes aleat√≥rias) e **exploitation** (a√ß√µes baseadas no aprendizado).  
- **Atualiza√ß√£o da Tabela Q**:  
  ```python
  Q(s, a) ‚Üê Q(s, a) + Œ± * [r + Œ≥ * max(Q(s', a')) - Q(s, a)]
  ```
  - `Œ±` (taxa de aprendizado): **0.2**  
  - `Œ≥` (fator de desconto): **0.95**  
  - `Œµ` (taxa de explora√ß√£o): Decai de **0.3** para **0.01** ao longo do treinamento.  

### **Discretiza√ß√£o do Estado**  
O estado do jogo √© representado por:  
1. **Dist√¢ncia horizontal** ao cano (`distancia_x // 50`).  
2. **Dist√¢ncia vertical** ao centro do v√£o (`distancia_centro_y // 25`).  
3. **Velocidade vertical** do p√°ssaro (`velocidade // 5`).  

---

## üìä C√°lculos e F√≥rmulas  
### **Recompensas**  
| A√ß√£o                     | Recompensa |  
|--------------------------|------------|  
| Passar por um cano       | `+100`     |  
| Bater em obst√°culo       | `-1000`    |  
| Nova pontua√ß√£o m√°xima    | `+100`     |  
| Sobreviv√™ncia por frame  | `+0.5`     |  

### **Decaimento da Explora√ß√£o**  
A cada epis√≥dio, `Œµ` √© reduzido multiplicando por **0.9995**, com valor m√≠nimo de **0.01**.  

---

## ‚ñ∂ Como Executar o Projeto  
1. **Clone o reposit√≥rio**:  
   ```bash
   git clone https://github.com/seu-usuario/FlappyBirdQLearning-01656165-StaloneAugusto.git
   cd FlappyBirdQLearning-01656165-StaloneAugusto
   ```

2. **Execute o jogo**:  
   ```bash
   python flappy_bird_qlearning.py
   ```

3. **Controles**:  
   - `ESC`: Encerra o programa.  
   - O agente toma decis√µes automaticamente.  

4. **Treinamento**:  
   - O agente come√ßa com alta explora√ß√£o (`Œµ = 0.3`).  
   - A cada 100 epis√≥dios, gr√°ficos de desempenho s√£o exibidos.  

---

## üìà Resultados e Coment√°rios Finais  
### **Desempenho Observado**  
- **Epis√≥dios iniciais**: 0‚Äì5 pontos (agente explora aleatoriamente).  
- **Ap√≥s ~500 epis√≥dios**: Pontua√ß√£o consistentemente acima de 10.  
- **Melhor pontua√ß√£o**: Entre 15‚Äì30 pontos.  

### **Gr√°ficos Gerados**  
1. **Recompensas por Epis√≥dio**: Mostra a evolu√ß√£o das recompensas.  
2. **Pontua√ß√£o por Epis√≥dio**: Indica a melhoria na estrat√©gia.  
3. **Taxa de Explora√ß√£o (Œµ)**: Decai ao longo do tempo.  

### **Imagens do Projeto em Execu√ß√£o**  

#### Tela do Jogo - Epis√≥dio 11  
![Epis√≥dio 11 - Explora√ß√£o: 0.1822, Pontos: 0](./imgs/FB/FB_1.PNG)  

#### Tela do Jogo - Epis√≥dio 27  
![Epis√≥dio 27 - Explora√ß√£o: 0.0868, Pontos: 1](./imgs/FB/FB_2.PNG)  

#### Gr√°ficos de Desempenho - Epis√≥dio 99  
![Gr√°ficos de Recompensas, Pontua√ß√£o e Explora√ß√£o](./imgs/FB/FB_3.PNG)  

### **Dificuldades Encontradas**  
- **Defini√ß√£o do espa√ßo de estados**: Encontrar a granularidade ideal exigiu ajustes.  
- **Sistema de recompensas**: Valores muito altos ou baixos levavam a comportamentos sub√≥timos.  
- **Tempo de treinamento**: O agente precisou de centenas de epis√≥dios para aprender.  

### **Conclus√£o**  
O projeto demonstra com sucesso a aplica√ß√£o do **Q-Learning** em um ambiente de jogo, destacando a import√¢ncia de:  
- Uma **boa discretiza√ß√£o do estado**.  
- **Recompensas/puni√ß√µes bem calibradas**.  
- **Paci√™ncia no treinamento** (o agente melhora gradualmente).  

**Pr√≥ximos passos**:  
- Implementar **Deep Q-Learning (DQN)** para lidar com espa√ßos de estado cont√≠nuos.  
- Adicionar mais m√©tricas de avalia√ß√£o (ex: taxa de sucesso por epis√≥dio).  

---

**Autor**: Stalone Augusto (Matr√≠cula: 01656165)  
**Reposit√≥rio**: [GitHub](https://github.com/seu-usuario/FlappyBirdQLearning-01656165-StaloneAugusto)